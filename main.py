import urllib.request
from urllib.parse import urlparse
import re
import os
from datetime import datetime, timedelta, timezone
import random
import time

# æ‰§è¡Œå¼€å§‹æ—¶é—´
timestart = datetime.now()

# è¯»å–æ–‡æœ¬æ–‡ä»¶åˆ°æ•°ç»„
def read_txt_to_array(file_name):
    try:
        with open(file_name, 'r', encoding='utf-8') as file:
            lines = file.readlines()
            lines = [line.strip() for line in lines]
            return lines
    except FileNotFoundError:
        print(f"æ–‡ä»¶æœªæ‰¾åˆ°: '{file_name}'")
        return []
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")
        return []

# é¢‘é“åˆ†ç±»
ys_lines = []  # å¤®è§†é¢‘é“
ws_lines = []  # å«è§†é¢‘é“
other_lines = []  # å…¶ä»–é¢‘é“

# é¢‘é“å­—å…¸ï¼ˆç”¨äºåˆ†ç±»ï¼‰
ys_dictionary = read_txt_to_array('ä¸»é¢‘é“/å¤®è§†é¢‘é“.txt')
ws_dictionary = read_txt_to_array('ä¸»é¢‘é“/å«è§†é¢‘é“.txt')

# è‡ªå®šä¹‰æº
urls = read_txt_to_array('assets/urls.txt')

# URLå“åº”æ—¶é—´ç¼“å­˜ï¼ˆé¿å…é‡å¤æµ‹é€Ÿï¼‰
speed_cache = {}

# æµ‹è¯•URLå“åº”æ—¶é—´
def test_url_speed(url, timeout=2):
    """
    æµ‹è¯•URLçš„å“åº”æ—¶é—´
    :param url: è¦æµ‹è¯•çš„URL
    :param timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    :return: å“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œå¦‚æœè¶…æ—¶æˆ–å‡ºé”™è¿”å›None
    """
    # æ£€æŸ¥ç¼“å­˜
    if url in speed_cache:
        return speed_cache[url]
    
    # åˆ›å»ºè¯·æ±‚å¯¹è±¡å¹¶æ·»åŠ è‡ªå®šä¹‰header
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
    
    try:
        start_time = time.time()
        req = urllib.request.Request(url, headers=headers)
        # æ‰“å¼€URLå¹¶è¯»å–å°‘é‡æ•°æ®ï¼ˆåªæµ‹è¯•è¿æ¥é€Ÿåº¦ï¼‰
        with urllib.request.urlopen(req, timeout=timeout) as response:
            # åªè¯»å–1KBæ•°æ®æ¥æµ‹è¯•è¿æ¥
            _ = response.read(1024)
        end_time = time.time()
        
        # è®¡ç®—å“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        response_time = (end_time - start_time) * 1000
        # ç¼“å­˜ç»“æœ
        speed_cache[url] = response_time
        return response_time
    except Exception as e:
        # å‡ºé”™æˆ–è¶…æ—¶
        speed_cache[url] = None
        return None

# M3Uæ ¼å¼åˆ¤æ–­
def is_m3u_content(text):
    """æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºM3Uæ ¼å¼"""
    lines = text.splitlines()
    if lines:
        first_line = lines[0].strip()
        return first_line.startswith("#EXTM3U")
    return False

# M3Uè½¬TXTæ ¼å¼
def convert_m3u_to_txt(m3u_content):
    """å°†M3Uå†…å®¹è½¬æ¢ä¸ºTXTæ ¼å¼"""
    lines = m3u_content.split('\n')
    txt_lines = []
    channel_name = ""
    
    for line in lines:
        if line.startswith("#EXTM3U"):
            continue
        if line.startswith("#EXTINF"):
            # è·å–é¢‘é“åç§°
            channel_name = line.split(',')[-1].strip()
        elif line.startswith("http") or line.startswith("rtmp") or line.startswith("p3p"):
            # è¿‡æ»¤æ— æ•ˆURL
            if "://" in line:
                txt_lines.append(f"{channel_name},{line.strip()}")
        # å¤„ç†éæ ‡å‡†æ ¼å¼
        if "#genre#" not in line and "," in line and "://" in line:
            pattern = r'^[^,]+,[^\s]+://[^\s]+$'
            if bool(re.match(pattern, line)):
                txt_lines.append(line)
    
    return '\n'.join(txt_lines)

# æ¸…ç†é¢‘é“åç§°
def clean_channel_name(channel_name):
    """æ¸…ç†é¢‘é“åç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦"""
    removal_list = ["ã€ŒIPV4ã€", "ã€ŒIPV6ã€", "[ipv6]", "[ipv4]", "_ç”µä¿¡", "ç”µä¿¡", 
                   "ï¼ˆHDï¼‰", "[è¶…æ¸…]", "é«˜æ¸…", "è¶…æ¸…", "-HD", "(HK)", "AKtv", "@", 
                   "IPV6", "ğŸï¸", "ğŸ¦", " ", "[BD]", "[VGA]", "[HD]", "[SD]", 
                   "(1080p)", "(720p)", "(480p)"]
    
    for item in removal_list:
        channel_name = channel_name.replace(item, "")
    
    # æ ‡å‡†åŒ–åç§°
    replacements = {
        "CCTV-": "CCTV",
        "CCTV0": "CCTV",
        "PLUS": "+",
        "NewTV-": "NewTV",
        "iHOT-": "iHOT",
        "NEW": "New",
        "New_": "New"
    }
    
    for old, new in replacements.items():
        channel_name = channel_name.replace(old, new)
    
    return channel_name.strip()

# å¤„ç†é¢‘é“è¡Œ
def process_channel_line(line):
    """
    å¤„ç†é¢‘é“è¡Œï¼Œåˆ†ç±»åˆ°å¤®è§†ã€å«è§†æˆ–å…¶ä»–
    å¹¶è¿›è¡Œä¸¥æ ¼æµ‹é€Ÿï¼ˆå“åº”æ—¶é—´<2ç§’ï¼‰
    """
    if "#genre#" not in line and "," in line and "://" in line:
        parts = line.split(',', 1)
        if len(parts) < 2:
            return
            
        channel_name = parts[0].strip()
        channel_url = parts[1].strip()
        
        # æ¸…ç†é¢‘é“åç§°
        channel_name = clean_channel_name(channel_name)
        
        # æµ‹è¯•URLå“åº”é€Ÿåº¦ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼‰
        response_time = test_url_speed(channel_url)
        
        # åªä¿ç•™å“åº”æ—¶é—´<2000msçš„æº
        if response_time is not None and response_time < 2000:
            # åˆ†ç±»å¤„ç†
            if channel_name in ys_dictionary:
                ys_lines.append(f"{channel_name},{channel_url}")
            elif channel_name in ws_dictionary:
                ws_lines.append(f"{channel_name},{channel_url}")
            else:
                other_lines.append(f"{channel_name},{channel_url}")

# å¤„ç†URLæº
def process_url(url):
    """å¤„ç†å•ä¸ªURLæº"""
    print(f"å¤„ç†URL: {url}")
    try:
        # åˆ›å»ºè¯·æ±‚å¯¹è±¡
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
        req = urllib.request.Request(url, headers=headers)
        
        # æ‰“å¼€URLå¹¶è¯»å–å†…å®¹
        with urllib.request.urlopen(req, timeout=10) as response:
            data = response.read()
            
            # å°è¯•ä¸åŒç¼–ç è§£ç 
            try:
                text = data.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    text = data.decode('gbk')
                except UnicodeDecodeError:
                    try:
                        text = data.decode('iso-8859-1')
                    except UnicodeDecodeError:
                        print("æ— æ³•ç¡®å®šåˆé€‚çš„ç¼–ç æ ¼å¼")
                        return
            
            # å¤„ç†M3Uæ ¼å¼
            if is_m3u_content(text):
                text = convert_m3u_to_txt(text)
            
            # é€è¡Œå¤„ç†å†…å®¹
            lines = text.split('\n')
            print(f"æ‰¾åˆ° {len(lines)} è¡Œå†…å®¹")
            
            for line in lines:
                # å¤„ç†å¸¦#çš„å¤šæºæ ¼å¼
                if "#genre#" not in line and "," in line and "://" in line:
                    if "#" not in line:
                        process_channel_line(line)
                    else:
                        parts = line.split(',', 1)
                        if len(parts) < 2:
                            continue
                            
                        channel_name = parts[0]
                        url_part = parts[1]
                        
                        # åˆ†å‰²å¤šä¸ªURL
                        url_list = url_part.split('#')
                        for channel_url in url_list:
                            if channel_url.strip():
                                process_channel_line(f"{channel_name},{channel_url}")

    except Exception as e:
        print(f"å¤„ç†URLæ—¶å‡ºé”™: {e}")

# å¤„ç†æ‰€æœ‰URLæº
for url in urls:
    if url.startswith("http"):
        process_url(url)

# ç”Ÿæˆæ—¶é—´æˆ³
beijing_time = datetime.now(timezone.utc) + timedelta(hours=8)
formatted_time = beijing_time.strftime("%Y%m%d %H:%M")
version = f"æ›´æ–°æ—¶é—´,#genre#\n{formatted_time},https://gcalic.v.myalicdn.com/gc/wgw05_1/index.m3u8?contentid=2820180516001"

# åˆå¹¶æ‰€æœ‰é¢‘é“æ•°æ®
all_lines = [version, '\n',
             "å¤®è§†é¢‘é“,#genre#"] + sorted(ys_lines) + ['\n',
             "å«è§†é¢‘é“,#genre#"] + sorted(ws_lines) + ['\n',
             "å…¶ä»–é¢‘é“,#genre#"] + sorted(other_lines)

# å†™å…¥æ–‡ä»¶
output_file = "live.txt"
try:
    with open(output_file, 'w', encoding='utf-8') as f:
        for line in all_lines:
            f.write(str(line) + '\n')
    print(f"é¢‘é“åˆ—è¡¨å·²ä¿å­˜åˆ°: {output_file}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"å¤®è§†é¢‘é“æ•°: {len(ys_lines)}")
    print(f"å«è§†é¢‘é“æ•°: {len(ws_lines)}")
    print(f"å…¶ä»–é¢‘é“æ•°: {len(other_lines)}")
    print(f"æ€»é¢‘é“æ•°: {len(ys_lines) + len(ws_lines) + len(other_lines)}")

except Exception as e:
    print(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")

# æ‰§è¡Œç»“æŸæ—¶é—´
timeend = datetime.now()

# è®¡ç®—æ‰§è¡Œæ—¶é—´
elapsed_time = timeend - timestart
total_seconds = elapsed_time.total_seconds()
minutes = int(total_seconds // 60)
seconds = int(total_seconds % 60)

print(f"æ‰§è¡Œæ—¶é—´: {minutes} åˆ† {seconds} ç§’")
